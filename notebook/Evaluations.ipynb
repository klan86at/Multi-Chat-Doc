{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a94e716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6661a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18916\\610711822.py:20: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  csv_path = \"D:\\LLMOPs\\Multi-Chat-Doc\\data\\goldens.csv\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# QA\n",
    "inputs = [\n",
    "    \"For customer-facing applications, which company's models dominate the top rankings?\",\n",
    "    \"What percentage of respondents are using RAG in some form?\",\n",
    "    \"How often are most respondents updating their models?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"OpenAI models dominate, with 3 of the top 5 and half of the top 10 most popular models for customer-facing apps.\",\n",
    "    \"70% of respondents are using RAG in some form.\",\n",
    "    \"More than 50% update their models at least monthly, with 17% doing so weekly.\",\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# write to csv\n",
    "csv_path = \"D:\\LLMOPs\\Multi-Chat-Doc\\data\\goldens.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed616bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['05365b30-c459-4f93-9ae0-7dd93830e3ff',\n",
       "  '30bba0de-984e-4fdb-bcfb-3e70e81e2920',\n",
       "  'bf165f72-78c3-4455-8fb5-59751637f711'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"llmops_dataset\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output for the AgenticAI report.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"\")\n",
    "\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from multi_doc_chat.src.document_chat.retrieval import ConversationalRAG\n",
    "import os\n",
    "\n",
    "# Simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"D:\\LLMOPs\\Multi-Chat-Doc\\data\\The 2025 AI Engineering Report.pdf\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided.\"}\n",
    "\n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"File not found: {data_path}\"}\n",
    "\n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "\n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base = \"data\",\n",
    "            faiss_base = \"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "\n",
    "        # Build retriever\n",
    "        ingestor.built_retriver(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        # Get session ID and Index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "\n",
    "        # Create RAG instance and load retriver\n",
    "        rag = ConversationalRAG(session_id=session_id)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "        )\n",
    "\n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "\n",
    "        return {\"answer\": answer}\n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"An error occurred: {str(e)}\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a sample question\n",
    "import test\n",
    "\n",
    "\n",
    "test_input = {\"question\": \"For customer-facing applications, which company's modell dominate the top rankings\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308207b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test with all gol[den questions\n",
    "print(\"Testing all questions from the dataset:\\n\")\n",
    "for i, q in enumerate(inputs, 1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\"A{i}: {result['answer']}\\n\")\n",
    "    print(\"*\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7044479",
   "metadata": {},
   "source": [
    "#### Implementing LangSmith in-built evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Evaluators\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"llmops_dataset\"\n",
    "\n",
    "# Run evaluation using our RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-llmops_dataset-qa-rag\",\n",
    "    # Experiment metadata\n",
    "    metadata = {\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multi-Chat-Doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
